{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7NlB8u4aQ72"
   },
   "source": [
    "# 1. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNVFNbIzaQ77"
   },
   "source": [
    "#### Business Problem Overview\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "\n",
    "\n",
    "For many incumbent operators, retaining high profitable customers is the number one business goal.\n",
    "\n",
    "To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n",
    "\n",
    "\n",
    "In this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\n",
    "\n",
    "\n",
    "Understanding and defining churn\n",
    "There are two main models of payment in the telecom industry - postpaid (customers pay a monthly/annual bill after using the services) and prepaid (customers pay/recharge with a certain amount in advance and then use the services).\n",
    "\n",
    "\n",
    "In the postpaid model, when customers want to switch to another operator, they usually inform the existing operator to terminate the services, and you directly know that this is an instance of churn.\n",
    "\n",
    "\n",
    "However, in the prepaid model, customers who want to switch to another network can simply stop using the services without any notice, and it is hard to know whether someone has actually churned or is simply not using the services temporarily (e.g. someone may be on a trip abroad for a month or two and then intend to resume using the services again).\n",
    "\n",
    "\n",
    "Thus, churn prediction is usually more critical (and non-trivial) for prepaid customers, and the term ‘churn’ should be defined carefully.  Also, prepaid is the most common model in India and Southeast Asia, while postpaid is more common in Europe in North America.\n",
    "\n",
    "This project is based on the Indian and Southeast Asian market.\n",
    "\n",
    "\n",
    "Definitions of churn\n",
    "There are various ways to define churn, such as:\n",
    "\n",
    "Revenue-based churn: Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc. over a given period of time. One could also use aggregate metrics such as ‘customers who have generated less than INR 4 per month in total/average/median revenue’.\n",
    "\n",
    " The main shortcoming of this definition is that there are customers who only receive calls/SMSes from their wage-earning counterparts, i.e. they don’t generate revenue but use the services. For example, many users in rural areas only receive calls from their wage-earning siblings in urban areas.\n",
    "\n",
    "\n",
    "Usage-based churn: Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time.\n",
    "\n",
    "\n",
    "A potential shortcoming of this definition is that when the customer has stopped using the services for a while, it may be too late to take any corrective actions to retain them. For e.g., if you define churn based on a ‘two-months zero usage’ period, predicting churn could be useless since by that time the customer would have already switched to another operator.\n",
    "\n",
    "\n",
    "\n",
    "In this project, you will use the usage-based definition to define churn.\n",
    "\n",
    "\n",
    "\n",
    "High-value churn\n",
    "In the Indian and the Southeast Asian market, approximately 80% of revenue comes from the top 20% customers (called high-value customers). Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.\n",
    "\n",
    "\n",
    "\n",
    "In this project, you will define high-value customers based on a certain metric (mentioned later below) and predict churn only on high-value customers.\n",
    "\n",
    "\n",
    "\n",
    "Understanding the business objective and the data\n",
    "The dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively.\n",
    "\n",
    "\n",
    "The business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.\n",
    "\n",
    "\n",
    "\n",
    "Understanding customer behaviour during churn\n",
    "Customers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers). In churn prediction, we assume that there are three phases of customer lifecycle :\n",
    "\n",
    "The ‘good’ phase: In this phase, the customer is happy with the service and behaves as usual.\n",
    "\n",
    "The ‘action’ phase: The customer experience starts to sore in this phase, for e.g. he/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the ‘good’ months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor’s offer/improving the service quality etc.)\n",
    "\n",
    "The ‘churn’ phase: In this phase, the customer is said to have churned. You define churn based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to you for prediction. Thus, after tagging churn as 1/0 based on this phase, you discard all data corresponding to this phase.\n",
    "\n",
    "\n",
    "In this case, since you are working over a four-month window, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month is the ‘churn’ phase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nc9TFYxtaQ7-"
   },
   "source": [
    "# 2. Importing Important Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hu_VUA2zaQ7_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKx-UvAZaQ7_"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaFy4Vp2aQ8A"
   },
   "outputs": [],
   "source": [
    "#Improting the PCA module\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLR--ni0aQ8A"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdTOeEtXaQ8A"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlHnU96yaQ8A"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bq4fbSGjaQ8A"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eOxVNJpGaQ8B"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vM5oZbomaQ8B"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNKlzfavaQ8B"
   },
   "source": [
    "# 3. Data Reading and Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fns8w0X9aQ8B"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\BA Assignment\\telecom_churn_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "12k8s9p6aQ8C"
   },
   "outputs": [],
   "source": [
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqfypK0jaQ8C"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zsVUFKZaQ8C"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekgxHvYWaQ8C"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ki8JqOjeaQ8C"
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ua1bwwwaQ8D"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LarDNJr2aQ8D"
   },
   "source": [
    "# 4. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lI_y9bpDaQ8D"
   },
   "outputs": [],
   "source": [
    "#copy the original data\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaV3GrUAaQ8D"
   },
   "source": [
    "#### We will create a utility function (missing_df) which will accept the given data frame and give back a dataframe sorted in descending order of missing value percentage for each columns in the data frame passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAWKytXFaQ8D",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def missing_df(dfname):\n",
    "    missing = round(100*(dfname.isnull().sum()/len(dfname.index)), 2)\n",
    "    missing = pd.DataFrame(missing).reset_index()\n",
    "    missing.columns = [\"ColumnName\",\"percentage\"]\n",
    "    missing = missing.sort_values(by=['percentage'],ascending=False)\n",
    "    return(missing)\n",
    "\n",
    "missdf = missing_df(data)\n",
    "missdf.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6HIBSfhaQ8D"
   },
   "source": [
    "#### If the recharge value is unavailable, it can be assumed that it has not been recharged. First, we will identify all recharge columns and impute them with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1ofHA9waQ8D",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recharge_col = []\n",
    "\n",
    "for col in list(data.columns):\n",
    "    if (('rech' in col) and ('date' not in col)):\n",
    "        recharge_col.append(col)\n",
    "\n",
    "recharge_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6RrFIQWaQ8E"
   },
   "outputs": [],
   "source": [
    "data[data.total_rech_data_6.isnull() == True][['total_rech_data_6','date_of_last_rech_data_6']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EE_H_00uaQ8E"
   },
   "source": [
    "#### missing recharge value also means that , they havnt recharged\n",
    "#### So will impute missing values as zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTV10QBWaQ8E",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[recharge_col].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHerYkqoaQ8E"
   },
   "source": [
    "#### some shows minimun recharge as 0 and some as 1, we can assume 0 for not recharging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1r-R1gk1aQ8E"
   },
   "outputs": [],
   "source": [
    "# impute missing values with 0\n",
    "data[recharge_col] = data[recharge_col].apply(lambda x: x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RpFnkmQ0aQ8E"
   },
   "outputs": [],
   "source": [
    "## data[recharge_col] =  data[recharge_col].apply(lambda x:0 if x is 1 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gW-pC_iCaQ8F"
   },
   "outputs": [],
   "source": [
    "data[recharge_col].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iw3V202haQ8F"
   },
   "outputs": [],
   "source": [
    "missdf = missing_df(data)\n",
    "missdf.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRKhPOSAaQ8F"
   },
   "outputs": [],
   "source": [
    "data['last_date_of_month_7'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wX4mntFCaQ8F"
   },
   "outputs": [],
   "source": [
    "data['last_date_of_month_6'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PKLlpmn6aQ8F"
   },
   "outputs": [],
   "source": [
    "data['last_date_of_month_7'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3CwkUEcaQ8F"
   },
   "outputs": [],
   "source": [
    "data['last_date_of_month_8'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8j1mVkZaQ8G"
   },
   "outputs": [],
   "source": [
    "data['last_date_of_month_9'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8lUoAleaQ8P"
   },
   "source": [
    "#### we will fill missing values of last_date_of_month date with actual last dates for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yu9U50xMaQ8P"
   },
   "outputs": [],
   "source": [
    "data['last_date_of_month_7'].fillna('7/31/2014',inplace = True)\n",
    "data['last_date_of_month_8'].fillna('8/31/2014',inplace = True)\n",
    "data['last_date_of_month_9'].fillna('9/30/2014',inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Hd1vZixaQ8P"
   },
   "source": [
    "#### Convert all date columns to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrLBhyG1aQ8P"
   },
   "outputs": [],
   "source": [
    "date_col = []\n",
    "\n",
    "for col in list(data.columns):\n",
    "    if ('date' in col):\n",
    "        date_col.append(col)\n",
    "\n",
    "date_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7q2UX_jaQ8P"
   },
   "outputs": [],
   "source": [
    "data[date_col] = data[date_col].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T5AnslFMaQ8Q"
   },
   "outputs": [],
   "source": [
    "data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JjGxnP5aQ8Q"
   },
   "source": [
    "#### will create a new column wich give how many days before the laste date of month recharge was done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQZ1rjHqaQ8Q"
   },
   "outputs": [],
   "source": [
    "data['duration_last_rech_6']= data['last_date_of_month_6']-data['date_of_last_rech_6']\n",
    "data['duration_last_rech_7']= data['last_date_of_month_7']-data['date_of_last_rech_7']\n",
    "data['duration_last_rech_8']= data['last_date_of_month_8']-data['date_of_last_rech_8']\n",
    "data['duration_last_rech_9']= data['last_date_of_month_9']-data['date_of_last_rech_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5RAJytEaQ8Q"
   },
   "outputs": [],
   "source": [
    "data['duration_last_rech_6']=data['duration_last_rech_6'].astype(str)\n",
    "data['duration_last_rech_6']=data['duration_last_rech_6'].str.split(\" \",n=3,expand=True)[0]\n",
    "data.loc[(data['duration_last_rech_6']=='NaT'),'duration_last_rech_6']=-1\n",
    "data['duration_last_rech_6']=data['duration_last_rech_6'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-p8cmzvaQ8Q"
   },
   "outputs": [],
   "source": [
    "data['duration_last_rech_7']=data['duration_last_rech_7'].astype(str)\n",
    "data['duration_last_rech_7']=data['duration_last_rech_7'].str.split(\" \",n=3,expand=True)[0]\n",
    "data.loc[(data['duration_last_rech_7']=='NaT'),'duration_last_rech_7']= -1\n",
    "data['duration_last_rech_7']=data['duration_last_rech_7'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aBUyQ8UaQ8Q"
   },
   "outputs": [],
   "source": [
    "data['duration_last_rech_8']=data['duration_last_rech_8'].astype(str)\n",
    "data['duration_last_rech_8']=data['duration_last_rech_8'].str.split(\" \",n=3,expand=True)[0]\n",
    "data.loc[(data['duration_last_rech_8']=='NaT'),'duration_last_rech_8']= -1\n",
    "data['duration_last_rech_8']=data['duration_last_rech_8'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4jKYJ3OaQ8Q"
   },
   "outputs": [],
   "source": [
    "data['duration_last_rech_9']=data['duration_last_rech_9'].astype(str)\n",
    "data['duration_last_rech_9']=data['duration_last_rech_9'].str.split(\" \",n=3,expand=True)[0]\n",
    "data.loc[(data['duration_last_rech_9']=='NaT'),'duration_last_rech_9']= -1\n",
    "data['duration_last_rech_9']=data['duration_last_rech_9'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inkuvn2vaQ8R"
   },
   "outputs": [],
   "source": [
    "data['duration_last_rech_data_6']= data['last_date_of_month_6']-data['date_of_last_rech_data_6']\n",
    "data['duration_last_rech_data_7']= data['last_date_of_month_7']-data['date_of_last_rech_data_7']\n",
    "data['duration_last_rech_data_8']= data['last_date_of_month_8']-data['date_of_last_rech_data_8']\n",
    "data['duration_last_rech_data_9']= data['last_date_of_month_9']-data['date_of_last_rech_data_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCv3jSNhaQ8R"
   },
   "outputs": [],
   "source": [
    "data['duration_last_rech_data_6']=data['duration_last_rech_data_6'].astype(str)\n",
    "data['duration_last_rech_data_6']=data['duration_last_rech_data_6'].str.split(\" \",n=3,expand=True)[0]\n",
    "data.loc[(data['duration_last_rech_data_6']=='NaT'),'duration_last_rech_data_6']= -1\n",
    "data['duration_last_rech_data_6']=data['duration_last_rech_data_6'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIddz_2haQ8R"
   },
   "outputs": [],
   "source": [
    "data['duration_last_rech_data_7']=data['duration_last_rech_data_7'].astype(str)\n",
    "data['duration_last_rech_data_7']=data['duration_last_rech_data_7'].str.split(\" \",n=3,expand=True)[0]\n",
    "data.loc[(data['duration_last_rech_data_7']=='NaT'),'duration_last_rech_data_7']= -1\n",
    "data['duration_last_rech_data_7']=data['duration_last_rech_data_7'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QeKsEG4caQ8R"
   },
   "outputs": [],
   "source": [
    "data['duration_last_rech_data_8']=data['duration_last_rech_data_8'].astype(str)\n",
    "data['duration_last_rech_data_8']=data['duration_last_rech_data_8'].str.split(\" \",n=3,expand=True)[0]\n",
    "data.loc[(data['duration_last_rech_data_8']=='NaT'),'duration_last_rech_data_8']= -1\n",
    "data['duration_last_rech_data_8']=data['duration_last_rech_data_8'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vq_iDHtCaQ8R"
   },
   "outputs": [],
   "source": [
    "data['duration_last_rech_data_9']=data['duration_last_rech_data_9'].astype(str)\n",
    "data['duration_last_rech_data_9']=data['duration_last_rech_data_9'].str.split(\" \",n=3,expand=True)[0]\n",
    "data.loc[(data['duration_last_rech_data_9']=='NaT'),'duration_last_rech_data_9']= -1\n",
    "data['duration_last_rech_data_9']=data['duration_last_rech_data_9'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQrJXloLaQ8R"
   },
   "source": [
    "#### Now we can drop the date columns as new meaningful columns are derrived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvqSXLpGaQ8S"
   },
   "outputs": [],
   "source": [
    "data.drop(date_col,axis=1,inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rQxxCOiaQ8S"
   },
   "outputs": [],
   "source": [
    "missdf = missing_df(data)\n",
    "missdf.set_index('ColumnName',inplace=True)\n",
    "missdf.head(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSSSQMzVaQ8S"
   },
   "outputs": [],
   "source": [
    "miss_col = missdf[missdf.percentage >0].index.tolist()\n",
    "miss_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h27JCct3aQ8S"
   },
   "source": [
    "#### Group MOU(Minutes of Usage ) and OTH (oter mobiles) columns to two lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbuLN_QJaQ8S"
   },
   "outputs": [],
   "source": [
    "cols_mou = []\n",
    "cols_other = []\n",
    "for col in miss_col:\n",
    "    if 'mou' in col:\n",
    "        cols_mou.append(col)\n",
    "    if 'others' in col:\n",
    "        cols_other.append(col)\n",
    "\n",
    "print(len(cols_mou), len(cols_other))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-i5qsyxvaQ8S"
   },
   "outputs": [],
   "source": [
    "cols_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "do4giSHxaQ8T"
   },
   "outputs": [],
   "source": [
    "data['ic_others_6'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ej0OPsx-aQ8T"
   },
   "outputs": [],
   "source": [
    "data['og_others_6'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVxVAGDhaQ8T"
   },
   "outputs": [],
   "source": [
    "data[data['og_others_6'].isnull() == True]['duration_last_rech_6'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oQC1O-maQ8T"
   },
   "source": [
    "#### we will inpute with 0 for all the missing columns as its the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjKC4YhAaQ8T"
   },
   "outputs": [],
   "source": [
    "data[cols_mou].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3KXZyrOaQ8U"
   },
   "outputs": [],
   "source": [
    "data[cols_other].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atbVXuLfaQ8U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UglNh8dsaQ8U"
   },
   "outputs": [],
   "source": [
    "for col in cols_mou:\n",
    "    data[col].fillna(0,inplace = True)\n",
    "\n",
    "for col in cols_other:\n",
    "    data[col].fillna(0,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3DhvdcQaQ8U"
   },
   "outputs": [],
   "source": [
    "missdf = missing_df(data)\n",
    "missdf.set_index('ColumnName',inplace=True)\n",
    "missdf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRMva5ZGaQ8U"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vZiuKueaQ8U"
   },
   "outputs": [],
   "source": [
    "data['night_pck_user_6'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfPRfX8oaQ8U"
   },
   "outputs": [],
   "source": [
    "#### we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-HeFG1XaQ8V"
   },
   "outputs": [],
   "source": [
    "# Fetching all categorical columns\n",
    "\n",
    "col_categorical=['night_pck_user_6','night_pck_user_7','night_pck_user_8','night_pck_user_9','fb_user_6','fb_user_7','fb_user_8','fb_user_9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KjMG5vUaQ8V"
   },
   "source": [
    "#### for categorical columns , missing value will set as new categorical value -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bq3p2A31aQ8V"
   },
   "outputs": [],
   "source": [
    "# Imputing missing categories as -1\n",
    "\n",
    "data[col_categorical]=data[col_categorical].apply(lambda x: x.fillna(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7C9q7FqnaQ8V"
   },
   "outputs": [],
   "source": [
    "missdf = missing_df(data)\n",
    "missdf.set_index('ColumnName',inplace=True)\n",
    "missdf.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "363uUmxzaQ8V"
   },
   "source": [
    "#### Average revenue per user, if missing will set to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VP4REv_jaQ8V"
   },
   "outputs": [],
   "source": [
    "arpu_col = []\n",
    "for col in missdf.index.tolist():\n",
    "    if 'arpu' in col and data[col].isnull().sum() > 0:\n",
    "        arpu_col.append(col)\n",
    "\n",
    "arpu_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8achOLCaQ8V"
   },
   "outputs": [],
   "source": [
    "# Imputing zeroes for all recharge columns\n",
    "\n",
    "data[arpu_col]=data[arpu_col].apply(lambda x: x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDz85Y1TaQ8V"
   },
   "outputs": [],
   "source": [
    "missdf = missing_df(data)\n",
    "missdf.set_index('ColumnName',inplace=True)\n",
    "missdf.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG9RU9iBaQ8W"
   },
   "source": [
    "# 5. Filter High Value customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGQC7AFIaQ8W"
   },
   "source": [
    "#### High-value churn\n",
    "#### In the Indian and the Southeast Asian market, approximately 80% of revenue comes from the top 20% customers (called high-value customers). Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.\n",
    "\n",
    "#### In this project, you will define high-value customers based on a certain metric (mentioned later below) and predict churn only on high-value customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90pIBzX6aQ8W"
   },
   "source": [
    "#### Good Phase (June & July)\n",
    "1. Calculate average recharge done by customer in June and July(total_rech_amt)\n",
    "2. Look at the 70th percentile recharge amount\n",
    "3. Retain only those customers who have recharged their mobiles with more than or equal to 70th percentile amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghhqp3kFaQ8W"
   },
   "source": [
    "#### Find the coulmns having recharge related info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yRLGIJYaQ8W"
   },
   "outputs": [],
   "source": [
    "recharge_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gra7G0rVaQ8W"
   },
   "outputs": [],
   "source": [
    "data[['av_rech_amt_data_6','count_rech_2g_6','count_rech_3g_6', 'total_rech_data_6','total_rech_amt_6','total_rech_num_6']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4K68Y8raQ8W"
   },
   "source": [
    "#### Step 1: We need to find the average recharge done by customer in the good phase\n",
    "####   For that, we will create a new column total recharge amount for data: ''total_rech_amt_data\"  , which can be calculated by multiplying the average recharge data with count of  data recharge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kU0g3u6PaQ8X"
   },
   "outputs": [],
   "source": [
    "data['total_rech_amt_data_6'] = data.av_rech_amt_data_6 * data.total_rech_data_6\n",
    "data['total_rech_amt_data_7'] = data.av_rech_amt_data_7 * data.total_rech_data_7\n",
    "data['total_rech_amt_data_8'] = data.av_rech_amt_data_8 * data.total_rech_data_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whGTXpkGaQ8X"
   },
   "source": [
    "####  Now to Calculate average recharge done by customer in June and July(total_rech_amt) months , we will calculate total recharge amount for  june and july months by adding total data recharge and call recharge for two months(june & july) and average it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqiq853-aQ8X"
   },
   "outputs": [],
   "source": [
    "data['total_avg_rech_amnt_good_phase'] = (data.total_rech_amt_6 + data.total_rech_amt_data_6 \\\n",
    "                                               + data.total_rech_amt_7+ data.total_rech_amt_data_7)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhNnqGXpaQ8X"
   },
   "source": [
    "#### Step 2: Look at 70th percentile of total_avg_rech_amnt_good_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JW7T-ox9aQ8X"
   },
   "outputs": [],
   "source": [
    "high_value_filter = data.total_avg_rech_amnt_good_phase.quantile(0.7)\n",
    "high_value_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTvhjRyPaQ8X"
   },
   "source": [
    "#### Step3: Retain only those customers who have recargted teir mobile more than or equal to 70th percentile amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9idTKLwSaQ8Y"
   },
   "outputs": [],
   "source": [
    "data_hvc = data[data.total_avg_rech_amnt_good_phase > high_value_filter]\n",
    "data_hvc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwBpqZsnaQ8Y"
   },
   "outputs": [],
   "source": [
    "data_hvc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diSroIrmaQ8Y"
   },
   "source": [
    "# 6. Derrive Churn ( Target Variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNe7hIdyaQ8Z"
   },
   "source": [
    "#### 9th Month is our Churn Phase. Usage-based churn\n",
    "#### we need to follow below steps to find the chhurn\n",
    "1. Calculate total incoming and outgoing minutes of usage\n",
    "2. Calculate 2g and 3g data consumption\n",
    "3. Create churn variable: those who have not used either calls or internet in the month of September are customers who have churned (churn =1, else 0)\n",
    "4. Check Churn percentage.\n",
    "5. Delete columns that belong to the churn month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pad8LfGOaQ8Z"
   },
   "source": [
    "#### First we will find the columns to be used for calculating churn . These will be columns which gives total incoming and outgoing minutes of usage and 2g & 3g data consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPl27P-paQ8Z"
   },
   "outputs": [],
   "source": [
    "churn_var_9 = []\n",
    "for col in data_hvc.columns.tolist():\n",
    "    if '_9' in col:\n",
    "        if 'ic_mou' or 'og_mou' or '_2g_' or '_3g_' in col:\n",
    "            churn_var_9.append(col)\n",
    "\n",
    "churn_var_9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRDk22leaQ8Z"
   },
   "source": [
    "####  'vol_2g_mb_9', 'vol_3g_mb_9','total_og_mou_9','total_ic_mou_9' => these columns can be used for calculating churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRFSTfUGaQ8Z"
   },
   "outputs": [],
   "source": [
    "hvc_9 = ['vol_2g_mb_9', 'vol_3g_mb_9','total_og_mou_9','total_ic_mou_9' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DEBbCxBaQ8Z"
   },
   "outputs": [],
   "source": [
    "df = data_hvc[hvc_9].reset_index(drop  = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lTO4WaIaQ8Z"
   },
   "outputs": [],
   "source": [
    "missdf = missing_df(df)\n",
    "missdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQNkDGOLaQ8a"
   },
   "source": [
    "####  step3: Create Churn Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aVND6MHaQ8a"
   },
   "outputs": [],
   "source": [
    "# Initially set all the values as 0\n",
    "data_hvc['churn']= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APhsVQCIaQ8a"
   },
   "outputs": [],
   "source": [
    "is_churned = (data_hvc.total_ic_mou_9 == 0) & \\\n",
    "             (data_hvc.total_og_mou_9 == 0) & \\\n",
    "             (data_hvc.vol_2g_mb_9 ==0) & \\\n",
    "             (data_hvc.vol_3g_mb_9 ==0)\n",
    "# set all which having is_churned True condition as 1\n",
    "data_hvc.loc[is_churned,'churn']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysUT1IbOaQ8a"
   },
   "source": [
    "#### Check the churn percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdogoLgeaQ8a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let us check what's the % of churned customers\n",
    "100*data_hvc.churn.sum()/len(data_hvc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j10qX73daQ8a"
   },
   "source": [
    "#### From  this it is evident that the dataset is highly imbalanced. The proportion for churn to non-churn is around 8%. For a correct and smooth analysis we need to deal with this class imbalance problem. We will deal with this in a later section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cvzkcyc7aQ8a"
   },
   "source": [
    "#### Delete the columns tat belongs to the churn month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GsCJK00raQ8b"
   },
   "outputs": [],
   "source": [
    "churn_col =  data_hvc.columns[data_hvc.columns.str.contains('_9')]\n",
    "churn_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oGB05W1QaQ8b"
   },
   "outputs": [],
   "source": [
    "# drop all columns corresponding to the churn phase\n",
    "data_hvc.drop(churn_col,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImjzWJd5aQ8b"
   },
   "source": [
    "# 7. Data Preparation & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5_3P9lQaQ8b"
   },
   "outputs": [],
   "source": [
    "data_hvc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOik8Z2XaQ8b"
   },
   "outputs": [],
   "source": [
    "data_hvc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYP-Im1EaQ8b"
   },
   "source": [
    "#### We can drop columns having unique identifiers and single values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--mFQ8shaQ8c"
   },
   "outputs": [],
   "source": [
    "unique_col = ['mobile_number','circle_id']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XR7EmopraQ8c"
   },
   "source": [
    "#### We will remove the columns aving only single value for all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AHmOzDh9aQ8c"
   },
   "outputs": [],
   "source": [
    "for col in data_hvc.columns.tolist():\n",
    "    if (len(data_hvc[col].unique().tolist()) ==1):\n",
    "        unique_col.append(col)\n",
    "\n",
    "unique_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWCyuzmLaQ8c"
   },
   "outputs": [],
   "source": [
    "data_hvc.drop(unique_col,axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8aX5f2mPaQ8c"
   },
   "outputs": [],
   "source": [
    "data_hvc.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPmvzKXuaQ8c"
   },
   "outputs": [],
   "source": [
    "data_hvc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p74RpIr0aQ8d"
   },
   "outputs": [],
   "source": [
    "data_hvc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2QvriD8aQ8d"
   },
   "outputs": [],
   "source": [
    "cat_col = []\n",
    "for col in data_hvc.columns.tolist():\n",
    "    if (len(data_hvc[col].unique().tolist()) < 17):\n",
    "        cat_col.append(col)\n",
    "        print(\"******************************* \\n\")\n",
    "        print(col)\n",
    "        print(data_hvc[col].unique().tolist())\n",
    "        print(\"******************************* \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysfiqgCnaQ8d"
   },
   "outputs": [],
   "source": [
    "print(len(cat_col) , cat_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Rnvv4wmaQ8d"
   },
   "outputs": [],
   "source": [
    "int_col = data_hvc.columns.tolist()\n",
    "for col in cat_col:\n",
    "    if col in int_col:\n",
    "        int_col.remove(col)\n",
    "\n",
    "int_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ilcaCENaQ8d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCwLkE1daQ8d"
   },
   "outputs": [],
   "source": [
    "int_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUnsXkGnaQ8e"
   },
   "outputs": [],
   "source": [
    "col_month_6 = []\n",
    "col_month_7 = []\n",
    "col_month_8 = []\n",
    "col_non_month = []\n",
    "\n",
    "for col in int_col:\n",
    "    if '_6' in col:\n",
    "        col_month_6.append(col)\n",
    "    elif '_7' in col:\n",
    "        col_month_7.append(col)\n",
    "    elif '_8' in col:\n",
    "        col_month_8.append(col)\n",
    "    else:\n",
    "        col_non_month.append(col)\n",
    "\n",
    "print(\"col_month_6, length \",len(col_month_6),col_month_6)\n",
    "print(\"\\n\")\n",
    "print(\"col_month_7, length \", len(col_month_7),col_month_7)\n",
    "print(\"\\n\")\n",
    "print(\"col_month_8, length \",len(col_month_8), col_month_8)\n",
    "print(\"\\n\")\n",
    "print(\"col_non_month, length \",len(col_non_month), col_non_month)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tICuVRfaQ8e"
   },
   "source": [
    "#### We will define some utility function below wich will help us to plot the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeXR8hbWaQ8e"
   },
   "outputs": [],
   "source": [
    "# Plot box plot for all columns in the list input_cols.\n",
    "def box_plot(data,input_cols,ncol):\n",
    "    leng = len(input_cols)\n",
    "    if leng%ncol == 0:\n",
    "        rows = leng//ncol\n",
    "    else:\n",
    "        rows = leng//ncol + 1\n",
    "\n",
    "    figure, axes = plt.subplots(nrows=rows, ncols=ncol,figsize=(20,3.5*rows))\n",
    "\n",
    "    for i, xvar in enumerate(input_cols):\n",
    "            axes[i//ncol,i%ncol].title.set_text(xvar)\n",
    "            axes[i//ncol,i%ncol].tick_params(axis='x', rotation=45)\n",
    "\n",
    "            axes[i//ncol,i%ncol].boxplot(data[xvar])\n",
    "\n",
    "\n",
    "\n",
    "    figure.tight_layout(pad=3.0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot histogram for all columns in the list input_cols.\n",
    "def plot_histogram(data, input_cols,ncol):\n",
    "    leng = len(input_cols)\n",
    "    if leng%ncol == 0:\n",
    "        rows = leng//ncol\n",
    "    else:\n",
    "        rows = leng//ncol + 1\n",
    "    fig = plt.figure(figsize=(20,3.5*rows))\n",
    "\n",
    "    for i, xvar in enumerate(input_cols):\n",
    "\n",
    "            fig.add_subplot(rows,ncol,i+1).tick_params(axis='x', rotation=45)\n",
    "            fig.add_subplot(rows,ncol,i+1).title.set_text(xvar + \" histogram\")\n",
    "            sns.distplot(data[xvar],hist = True)\n",
    "\n",
    "    fig.tight_layout(pad=3.0)\n",
    "\n",
    "\n",
    "# create box plot for  6th, 7th and 8th month\n",
    "def plot_box_chart(attribute):\n",
    "    plt.figure(figsize=(20,16))\n",
    "    df = data_hvc\n",
    "    plt.subplot(2,3,1)\n",
    "    sns.boxplot(data=df, y=attribute+\"_6\",x=\"churn\",hue=\"churn\",\n",
    "                showfliers=False,palette=(\"magma\"))\n",
    "    plt.subplot(2,3,2)\n",
    "    sns.boxplot(data=df, y=attribute+\"_7\",x=\"churn\",hue=\"churn\",\n",
    "                showfliers=False,palette=(\"magma\"))\n",
    "    plt.subplot(2,3,3)\n",
    "    sns.boxplot(data=df, y=attribute+\"_8\",x=\"churn\",hue=\"churn\",\n",
    "                showfliers=False,palette=(\"magma\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Custom Function for Default Plotting variables\n",
    "\n",
    "# Function Parameters  -\n",
    "\n",
    "# figure_title         -    The title to use for the plot.\n",
    "# xlabel               -    The x-axis label for the plot.\n",
    "# ylabel               -    The y-axis label for the plot.\n",
    "\n",
    "def set_plotting_variable(figure_title, xlabel, ylabel):\n",
    "\n",
    "    plt.title(figure_title)\n",
    "    plt.xlabel(xlabel, labelpad = 15)\n",
    "    plt.ylabel(ylabel, labelpad = 10)\n",
    "\n",
    "\n",
    "\n",
    "# Custom Function to add data labels in the graph\n",
    "\n",
    "def add_data_labels(ax, spacing = 5):\n",
    "\n",
    "    # For each bar: Place a label\n",
    "    for rect in ax.patches:\n",
    "        # Get X and Y placement of label from rect.\n",
    "        y_value = rect.get_height()\n",
    "        x_value = rect.get_x() + rect.get_width() / 2\n",
    "\n",
    "        # Number of points between bar and label. Change to your liking.\n",
    "        space = spacing\n",
    "        # Vertical alignment for positive values\n",
    "        va = 'bottom'\n",
    "\n",
    "        # If value of bar is negative: Place label below bar\n",
    "        if y_value < 0:\n",
    "            # Invert space to place label below\n",
    "            space *= -1\n",
    "            # Vertically align label at top\n",
    "            va = 'top'\n",
    "\n",
    "        # Use Y value as label and format number with one decimal place\n",
    "        label = \"{:.2f}%\".format(y_value)\n",
    "\n",
    "        # Create annotation\n",
    "        plt.annotate(\n",
    "            label,                        # Use `label` as label\n",
    "            (x_value, y_value),           # Place label at end of the bar\n",
    "            xytext = (0, space),          # Vertically shift label by `space`\n",
    "            textcoords = \"offset points\", # Interpret `xytext` as offset in points\n",
    "            ha = 'center',                # Horizontally center label\n",
    "            va = va)                      # Vertically align label differently for positive and negative values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Custom Function for Univariate Analysis\n",
    "\n",
    "# Function Parameters   -\n",
    "\n",
    "# figsize_x             -      The width of the plot figure in inches.\n",
    "# figsize_y             -      The height of the plot figure in inches.\n",
    "# subplot_x             -      The rows for the subplot.\n",
    "# subplot_y             -      The columns for the subplot.\n",
    "# xlabel                -      The x-axis label for the plot.\n",
    "# ylabel                -      The y-axis label for the plot.\n",
    "# x_axis                -      The series/variable to be plotted along the x-axis.\n",
    "# data                  -      The data frame.\n",
    "\n",
    "# wspace                -      The amount of width reserved for space between subplots,\n",
    "#                              expressed as a fraction of the average axis width\n",
    "\n",
    "# xlabel_rotation       -      The degree of rotation for the x-axis ticks (values).\n",
    "\n",
    "def plot_univariate(figsize_x, figsize_y, subplot_x, subplot_y, xlabel, ylabel, x_axis, data, wspace):\n",
    "\n",
    "    plt.figure(figsize = (figsize_x, figsize_y))\n",
    "\n",
    "    title_1 = \"Distribution Plot of \" + xlabel\n",
    "    title_2 = \"Box Plot of \" + xlabel\n",
    "\n",
    "    # Subplot - 1\n",
    "    plt.subplot(subplot_x, subplot_y, 1)\n",
    "\n",
    "    sns.distplot(data[x_axis], hist = True, kde = True, color = 'g')\n",
    "    # Call Custom Function\n",
    "    set_plotting_variable(title_1, xlabel, ylabel)\n",
    "\n",
    "    # Subplot - 2\n",
    "    plt.subplot(subplot_x, subplot_y, 2)\n",
    "\n",
    "    sns.boxplot(x = x_axis, data = data, color = 'm')\n",
    "    # Call Custom Function\n",
    "    set_plotting_variable(title_2, xlabel, ylabel)\n",
    "\n",
    "    plt.subplots_adjust(wspace = wspace)\n",
    "    plt.show()\n",
    "\n",
    "# Custom Function for Bivariate Analysis\n",
    "\n",
    "# Function Parameters   -\n",
    "\n",
    "# y_axis                -      The series/variable to be plotted along the y-axis.\n",
    "\n",
    "def plot_bivariate(y_axis):\n",
    "\n",
    "    plt.figure(figsize = (15, 5))\n",
    "\n",
    "    xlabel = \"Churn\"\n",
    "    x_axis = \"churn\"\n",
    "\n",
    "    title_1 = \"Month 6 - \" + xlabel\n",
    "    title_2 = \"Month 7 - \" + xlabel\n",
    "    title_3 = \"Month 8 - \" + xlabel\n",
    "\n",
    "    print(\"\\nData Visualization of churn vs \" + y_axis)\n",
    "\n",
    "    # Subplot - 1\n",
    "    plt.subplot(1, 3, 1)\n",
    "\n",
    "    sns.boxplot(x = x_axis, y = y_axis + \"_6\", hue = \"churn\", data = data_hvc, showfliers = False)\n",
    "    #sns.barplot(x = x_axis, y = y_axis + \"_6\", hue = \"churn\", data = data_hvc)\n",
    "    # Call Custom Function\n",
    "    set_plotting_variable(title_1, xlabel, y_axis + \"_6\")\n",
    "\n",
    "    # Subplot - 2\n",
    "    plt.subplot(1, 3, 2)\n",
    "\n",
    "    sns.boxplot(x = x_axis, y = y_axis + \"_7\", hue = \"churn\", data = data_hvc, showfliers = False)\n",
    "    #sns.barplot(x = x_axis, y = y_axis + \"_7\", hue = \"churn\", data = data_hvc)\n",
    "    # Call Custom Function\n",
    "    set_plotting_variable(title_2, xlabel, y_axis + \"_7\")\n",
    "\n",
    "    # Subplot - 3\n",
    "    plt.subplot(1, 3, 3)\n",
    "\n",
    "    sns.boxplot(x = x_axis, y = y_axis + \"_8\", hue = \"churn\", data = data_hvc, showfliers = False)\n",
    "    #sns.barplot(x = x_axis, y = y_axis + \"_8\", hue = \"churn\", data = data_hvc)\n",
    "    # Call Custom Function\n",
    "    set_plotting_variable(title_3, xlabel, y_axis + \"_8\")\n",
    "\n",
    "    plt.subplots_adjust(wspace = 0.4)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgledm37aQ8e"
   },
   "source": [
    "#### Univariate Plot Analysis of Quantitative Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AJ4P0lRaQ8f"
   },
   "source": [
    "#### Boxplot non Monthly columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6a4RCNcEaQ8f"
   },
   "outputs": [],
   "source": [
    "# INT Data\n",
    "a = 3  # number of rows\n",
    "b = 2 # number of columns\n",
    "c = 1  # initialize plot counter\n",
    "\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "\n",
    "for i in col_non_month:\n",
    "    plt.subplot(a, b, c)\n",
    "    plt.title('{}, subplot: {}{}{}'.format(i, a, b, c))\n",
    "    plt.xlabel(i)\n",
    "    sns.boxplot(data_hvc[i])\n",
    "    c = c + 1\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo7Vxgu5aQ8f"
   },
   "source": [
    "#### Observation\n",
    "\n",
    " There are outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWljXyP1aQ8f"
   },
   "outputs": [],
   "source": [
    "\n",
    "counter = 1\n",
    "\n",
    "for col_list in data_hvc.columns:\n",
    "\n",
    "    if col_list not in cat_col:\n",
    "\n",
    "        # Call Custom Function\n",
    "        plot_univariate(figsize_x = 20,\n",
    "                        figsize_y = 8,\n",
    "                        subplot_x = 1,\n",
    "                        subplot_y = 2,\n",
    "                        xlabel = col_list,\n",
    "                        ylabel = \"Distribution\",\n",
    "                        x_axis = col_list,\n",
    "                        data = data_hvc,\n",
    "                        wspace = 0.2)\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaQoSXmWaQ8f"
   },
   "source": [
    "#### There are lot of Outliers present in the variables. We will remove these outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIFJDTHiaQ8f"
   },
   "source": [
    "#### some of the arpu (average revenue per user) value is less than zero\n",
    "#### Now the revenue generated from a user cannot be negative. If a customer is not using any services then apru for the person would be zero (rather that being negative). Now if arpu is negative for any row, then that would mean that is a wrong/corrupt data. It will be of no use to us for analysis. We will drop such observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUVvJdaBaQ8f"
   },
   "outputs": [],
   "source": [
    "# Index where the arpu values for month 6 are less than 0 -\n",
    "\n",
    "arpu_6_index = (data_hvc['arpu_6'] < 0)\n",
    "\n",
    "# Total number of such observations for month 6 -\n",
    "print('Total observations with negative arpu values for month 6 -', arpu_6_index.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yiu6gwd1aQ8g"
   },
   "outputs": [],
   "source": [
    "# Index where the arpu values for month 7 are less than 0 -\n",
    "\n",
    "arpu_7_index = (data_hvc['arpu_7'] < 0)\n",
    "\n",
    "# Total number of such observations for month 6 -\n",
    "print('Total observations with negative arpu values for month 7 -', arpu_7_index.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhL6_wlRaQ8g"
   },
   "outputs": [],
   "source": [
    "# Index where the arpu values for month 8 are less than 0 -\n",
    "\n",
    "arpu_8_index = (data_hvc['arpu_8'] < 0)\n",
    "\n",
    "# Total number of such observations for month 6 -\n",
    "print('Total observations with negative arpu values for month 8 -', arpu_8_index.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySIvfetbaQ8g"
   },
   "source": [
    "#### we will delete these wrong observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4i9jNeOlaQ8g"
   },
   "outputs": [],
   "source": [
    "# Let's delete the observations with negative arpu values.\n",
    "\n",
    "data_hvc = data_hvc[(data_hvc['arpu_6'] >= 0) & (data_hvc['arpu_7'] >= 0) & (data_hvc['arpu_8'] >= 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5un-3M-1aQ8h"
   },
   "source": [
    "#### Now we will remove te outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1uN-pMfaQ8h"
   },
   "source": [
    "#### We will Cap outliers in all numeric variables with k-sigma technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BP8cJtHsaQ8h"
   },
   "outputs": [],
   "source": [
    "def cap_outliers(array, k=3):\n",
    "    upper_limit = array.mean() + k*array.std()\n",
    "    lower_limit = array.mean() - k*array.std()\n",
    "    array[array<lower_limit] = lower_limit\n",
    "    array[array>upper_limit] = upper_limit\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDZTAZD5aQ8h"
   },
   "outputs": [],
   "source": [
    "\n",
    "# cap outliers in the numeric columns\n",
    "data_hvc[int_col] = data_hvc[int_col].apply(cap_outliers, axis=0)\n",
    "data_hvc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8jUtWAYaQ8h"
   },
   "source": [
    "#### we will plot again and confirm now no outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEkhLRyDaQ8i"
   },
   "outputs": [],
   "source": [
    "\n",
    "counter = 1\n",
    "\n",
    "for col_list in data_hvc.columns:\n",
    "\n",
    "    if col_list not in cat_col:\n",
    "\n",
    "        # Call Custom Function\n",
    "        plot_univariate(figsize_x = 20,\n",
    "                        figsize_y = 8,\n",
    "                        subplot_x = 1,\n",
    "                        subplot_y = 2,\n",
    "                        xlabel = col_list,\n",
    "                        ylabel = \"Distribution\",\n",
    "                        x_axis = col_list,\n",
    "                        data = data_hvc,\n",
    "                        wspace = 0.2)\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RU56cqrRaQ8i"
   },
   "outputs": [],
   "source": [
    "plot_histogram(data_hvc,col_non_month,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agm3yDroaQ8i"
   },
   "source": [
    "#### Bivariate for category columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjmzZmU9aQ8i"
   },
   "outputs": [],
   "source": [
    "for col in cat_col:\n",
    "    data_hvc.groupby(col)['total_avg_rech_amnt_good_phase'].sum().plot.barh()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4B475Kz_aQ8i"
   },
   "source": [
    "#### Observation :\n",
    "\n",
    "- people who haven't selected night_pck,  monthly_2g,monthly_3g  are spending more average amount\n",
    "- People selected fb_user are spending more average amount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXSdkgjWaQ8i"
   },
   "source": [
    "####  Bivariate Plot Analysis of Ordered categorical variables vs Percentage Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCfnArSraQ8i"
   },
   "source": [
    "#### we will plot each using boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u53saqwYaQ8i"
   },
   "outputs": [],
   "source": [
    "for i in cat_col:\n",
    "    fig = plt.figure(figsize=(15,11))\n",
    "    sns.boxplot(data=data_hvc,x=i,y=\"total_avg_rech_amnt_good_phase\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nL7tUn4aaQ8j"
   },
   "outputs": [],
   "source": [
    "cat_col.remove('churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbEKt5mlaQ8j"
   },
   "outputs": [],
   "source": [
    "counter = 1\n",
    "\n",
    "plt.figure(figsize = (15, 12))\n",
    "\n",
    "for col_list in cat_col:\n",
    "\n",
    "    series = round(((data_hvc[col_list].value_counts(dropna = False))/(len(data_hvc[col_list])) * 100), 2)\n",
    "\n",
    "    plt.subplot(4, 4, counter)\n",
    "    ax = sns.barplot(x = series.index, y = series.values, order = series.sort_index().index)\n",
    "    plt.xlabel(col_list, labelpad = 15)\n",
    "    plt.ylabel('Percentage Rate', labelpad = 10)\n",
    "\n",
    "    # Call Custom Function\n",
    "    add_data_labels(ax)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "del counter, ax\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.3)\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xG0dUeOtaQ8j"
   },
   "source": [
    "#### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayzKDTC8aQ8j"
   },
   "outputs": [],
   "source": [
    "# Bivariate Analysis\n",
    "\n",
    "plot_bivariate(\"arpu\")\n",
    "\n",
    "plot_bivariate(\"onnet_mou\")\n",
    "\n",
    "plot_bivariate(\"offnet_mou\")\n",
    "\n",
    "plot_bivariate(\"total_og_mou\")\n",
    "\n",
    "plot_bivariate(\"total_ic_mou\")\n",
    "\n",
    "plot_bivariate(\"total_rech_num\")\n",
    "\n",
    "plot_bivariate(\"total_rech_amt\")\n",
    "\n",
    "plot_bivariate(\"total_rech_data\")\n",
    "\n",
    "plot_bivariate(\"vol_2g_mb\")\n",
    "\n",
    "plot_bivariate(\"vol_3g_mb\")\n",
    "\n",
    "plot_bivariate(\"total_rech_amt_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS93P4Y_aQ8j"
   },
   "source": [
    "#### here is a significant drop in the columns for data in 8th month for churned customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btkRDpfjaQ8j"
   },
   "source": [
    "####  Looking at the problem statement, attributes total_ic_mou_6, total_og_mou_6, vol_2g_mb_ 6and vol_3g_mb_ 6are used to tag churn. So it is evident from the problem statement that the individual incoming and outgoing attributes are not used for data analysis. Dropping the individual columns (whose totals are already available like incoming, outgoing, arpu, etc) can help us in better analysis. Also, dropping these individual columns will help in removing the multicollinearity.\n",
    "\n",
    "#### Let's now drop all those individual columns whose totals are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4aGc0CphaQ8j"
   },
   "outputs": [],
   "source": [
    "# Let's drop individual columns whose totals are available as a different attribute\n",
    "\n",
    "single_cols = ['loc_ic_t2t_mou_6', 'loc_ic_t2t_mou_7', 'loc_ic_t2t_mou_8',\n",
    "                   'loc_ic_t2m_mou_6', 'loc_ic_t2m_mou_7', 'loc_ic_t2m_mou_8',\n",
    "                   'loc_ic_t2f_mou_6', 'loc_ic_t2f_mou_7', 'loc_ic_t2f_mou_8',\n",
    "                   'std_ic_t2t_mou_6', 'std_ic_t2t_mou_7', 'std_ic_t2t_mou_8',\n",
    "                   'std_ic_t2m_mou_6', 'std_ic_t2m_mou_7', 'std_ic_t2m_mou_8',\n",
    "                   'std_ic_t2f_mou_6', 'std_ic_t2f_mou_7', 'std_ic_t2f_mou_8',\n",
    "                   'loc_og_t2t_mou_6', 'loc_og_t2t_mou_7', 'loc_og_t2t_mou_8',\n",
    "                   'loc_og_t2m_mou_6', 'loc_og_t2m_mou_7', 'loc_og_t2m_mou_8',\n",
    "                   'loc_og_t2f_mou_6', 'loc_og_t2f_mou_7', 'loc_og_t2f_mou_8',\n",
    "                   'loc_og_t2c_mou_6', 'loc_og_t2c_mou_7', 'loc_og_t2c_mou_8',\n",
    "                   'std_og_t2t_mou_6', 'std_og_t2t_mou_7', 'std_og_t2t_mou_8',\n",
    "                   'std_og_t2m_mou_6', 'std_og_t2m_mou_7', 'std_og_t2m_mou_8',\n",
    "                   'std_og_t2f_mou_6', 'std_og_t2f_mou_7', 'std_og_t2f_mou_8',\n",
    "                   'last_day_rch_amt_6', 'last_day_rch_amt_7', 'last_day_rch_amt_8',\n",
    "                   'arpu_3g_6', 'arpu_3g_7', 'arpu_3g_8',\n",
    "                   'arpu_2g_6', 'arpu_2g_7', 'arpu_2g_8',\n",
    "                   'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8']\n",
    "\n",
    "data_hvc[single_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMspiUhuaQ8k"
   },
   "outputs": [],
   "source": [
    "data_hvc.drop(single_cols, axis = 1, inplace = True)\n",
    "\n",
    "data_hvc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZiQ9i32aQ8k"
   },
   "outputs": [],
   "source": [
    "corr_matrix = data_hvc.corr().abs()\n",
    "\n",
    "#the matrix is symmetric so we need to extract upper triangle matrix without diagonal (k = 1)\n",
    "upper_triangle = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)))\n",
    "\n",
    "highly_correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.80)]\n",
    "print(\"List of highly correlated features from the above plot - \\n\\n\", highly_correlated_features)\n",
    "print(\"\\n\\nTotal features with high correlation - \", len(highly_correlated_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47LB1ipraQ8k"
   },
   "outputs": [],
   "source": [
    "data_hvc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xqgsSOpaQ8k"
   },
   "outputs": [],
   "source": [
    "data_hvc.reset_index(drop=True)\n",
    "data_hvc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pXcumaFaQ8l"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(11,9))\n",
    "sns.heatmap(data_hvc[cat_col].corr(),annot=True,cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0WpjNnDaQ8l"
   },
   "source": [
    "#### night_pack_user & fb user are highly correlated. So we can drop one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3pVcvUFaQ8l"
   },
   "outputs": [],
   "source": [
    "cat_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsrxA_wvaQ8l"
   },
   "outputs": [],
   "source": [
    "data_hvc.drop(['night_pck_user_6','night_pck_user_7','night_pck_user_8'],axis=1,inplace=True)\n",
    "data_hvc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neSl3g2JaQ8m"
   },
   "outputs": [],
   "source": [
    "mou_col = []\n",
    "for col in data_hvc.columns.tolist():\n",
    "    if 'mou' in col and col:\n",
    "        mou_col.append(col)\n",
    "\n",
    "len(mou_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZcKlx8UaQ8m"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(20,11))\n",
    "sns.heatmap(data_hvc[mou_col].corr(),annot=True,cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBJ7CgPHaQ8m"
   },
   "source": [
    "#### high correlation bwn -> total_ic_mou_6' & loc_ic_mou_6, total_ic_mou_7' & loc_ic_mou_7, total_ic_mou_8' & loc_ic_mou_8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvtHABO9aQ8m"
   },
   "outputs": [],
   "source": [
    "data_hvc.drop(['loc_ic_mou_6','loc_ic_mou_7','loc_ic_mou_8'],axis=1,inplace=True)\n",
    "\n",
    "data_hvc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atLPhClwaQ8m"
   },
   "source": [
    "#### check correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IruPgtj_aQ8m"
   },
   "outputs": [],
   "source": [
    "data_hvc.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8CLco3v-aQ8n"
   },
   "outputs": [],
   "source": [
    "corr_matrix = data_hvc.corr().abs()\n",
    "\n",
    "#the matrix is symmetric so we need to extract upper triangle matrix without diagonal (k = 1)\n",
    "upper_triangle = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)))\n",
    "\n",
    "highly_correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.80)]\n",
    "print(\"List of highly correlated features from the above plot - \\n\\n\", highly_correlated_features)\n",
    "print(\"\\n\\nTotal features with high correlation - \", len(highly_correlated_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMcKnbC-aQ8n"
   },
   "source": [
    "#### Number of highly correlated features have reduced. Now we will leave this high correlation as it is  since we will use PCA and other techniques to remove it in later part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZIu9IPgaQ8n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l92Q7wEHaQ8n"
   },
   "source": [
    "### Deriving new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QR4iB7veaQ8n"
   },
   "source": [
    "#### Understanding customer behaviour during churn\n",
    "##### The churn prediction model assumes that customers switch to competitors over time, particularly high-value ones, and involves three phases of the customer lifecycle.\n",
    "\n",
    "##### The ‘good’ phase: In this phase, the customer is happy with the service and behaves as usual.\n",
    "\n",
    "##### During the 'action' phase, customers experience discomfort, may face competitor offers, unjust charges, or poor service quality, necessitating corrective actions to improve their experience.\n",
    "\n",
    "##### The 'churn' phase refers to a customer's behavior during which data is not available for prediction, and after tagging it as 1/0, all data is discarded.\n",
    "\n",
    " ##### In this case, since you are working over a four-month window, the first two months are the ‘good’ phase, the third month is the ‘action’ phase, while the fourth month is the ‘churn’ phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgnlRFUKaQ8n"
   },
   "source": [
    "#### So we will create some new fields by combining  some of the data for good phase and taking average of it , also by finding the difference from good phase and action phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bV2jRKxtaQ8o"
   },
   "outputs": [],
   "source": [
    "# we will define a Custom Function to derive new good phase columns (combining 6 & 7 months) and drop the original columns\n",
    "def derive_good_action_phase(df, col):\n",
    "\n",
    "    col_6 = col + \"_6\"\n",
    "    col_7 = col + \"_7\"\n",
    "    col_8 = col + \"_8\"\n",
    "    good_phase_col = col + \"_good_phase\"\n",
    "    action_phase_col = col + \"_action_phase\"\n",
    "\n",
    "    df[good_phase_col] = (df[col_6] + df[col_7])/2\n",
    "    df[action_phase_col] = df[col_8] - df[good_phase_col]\n",
    "\n",
    "    df.drop([col_6, col_7, col_8], axis = 1, inplace = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVJEIr1YaQ8o"
   },
   "source": [
    "#### Copying to final_data just to restore in case of any issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yyGsDRZaQ8p"
   },
   "outputs": [],
   "source": [
    "final_data = data_hvc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmv8OsMraQ8p"
   },
   "outputs": [],
   "source": [
    "feature_list = ['arpu',\"onnet_mou\",\"offnet_mou\",\"roam_ic_mou\",\"roam_og_mou\",\"loc_og_mou\",\"std_og_mou\",\"isd_og_mou\",\"spl_og_mou\",\n",
    "                \"og_others\",\"total_og_mou\",\"std_ic_mou\",\"spl_ic_mou\",\"isd_ic_mou\",\"ic_others\",\"total_ic_mou\",\n",
    "               \"total_rech_num\",\"total_rech_amt\",\"max_rech_amt\",\"total_rech_data\",\"max_rech_data\",\"count_rech_2g\",\"count_rech_3g\",\n",
    "               \"vol_2g_mb\",\"vol_3g_mb\",\"monthly_2g\",\"sachet_2g\",\"monthly_3g\",\"sachet_3g\",\"total_rech_amt_data\"]\n",
    "\n",
    "for col in feature_list:\n",
    "    data_hvc = derive_good_action_phase(data_hvc, col)\n",
    "\n",
    "data_hvc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKv1hh9BaQ8q"
   },
   "outputs": [],
   "source": [
    "data_hvc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xD_lQXuHaQ8q"
   },
   "outputs": [],
   "source": [
    "\n",
    "data_hvc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e1zxTT0caQ8r"
   },
   "outputs": [],
   "source": [
    "len(data_hvc.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEkKYFmTaQ8s"
   },
   "source": [
    "# 8 .Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIzxcYUDaQ8s"
   },
   "outputs": [],
   "source": [
    "# Separate input features and target\n",
    "y = data_hvc.churn\n",
    "X = data_hvc.drop('churn', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYLSefAcaQ8s"
   },
   "outputs": [],
   "source": [
    "cols = X.columns\n",
    "X = pd.DataFrame(scale(X))\n",
    "X.columns = cols\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bEEUuR-aQ8s"
   },
   "outputs": [],
   "source": [
    "# setting up testing and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPoF7jg3aQ8t"
   },
   "source": [
    "### Handling Class Imbalance using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7H9zFCJgaQ8u"
   },
   "outputs": [],
   "source": [
    "data_hvc['churn'].value_counts().plot(kind= 'bar').set_title('churned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6bkgcA9uaQ8u"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "da2TdbJTaQ8v"
   },
   "outputs": [],
   "source": [
    "\n",
    "smote  =  SMOTE()\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLWRtFMQaQ8v"
   },
   "source": [
    "#### we will check if data looks balanced now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZX5G13QcaQ8v"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train).churn.value_counts().plot(kind= 'bar').set_title('churned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tPx6WLwaQ8v"
   },
   "source": [
    "####  now both churned and non churned data is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6x2hsitaQ8v"
   },
   "source": [
    "### Performing PCA for feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbhWVT--aQ8v"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6RItYOEaQ8w"
   },
   "outputs": [],
   "source": [
    "pca =  PCA(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MdXAjzdaQ8w"
   },
   "outputs": [],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4KiRAp86aQ8w"
   },
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vcNAOtnJaQ8w"
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnB5qZypaQ8w"
   },
   "outputs": [],
   "source": [
    "var_cumu = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ipafXiJpaQ8w"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[12,8])\n",
    "plt.vlines(x=15, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\n",
    "plt.hlines(y=0.95, xmax=70, xmin=0, colors=\"g\", linestyles=\"--\")\n",
    "plt.plot(var_cumu)\n",
    "plt.ylabel(\"Cumulative variance explained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmI22zXZaQ8w"
   },
   "outputs": [],
   "source": [
    "### As per this Using 16 Variable can explain 90% of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUzxnsInaQ8w"
   },
   "outputs": [],
   "source": [
    "pca_final = IncrementalPCA(n_components=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FpklSun_aQ8x"
   },
   "outputs": [],
   "source": [
    "df_train_pca = pca_final.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSfrwgdbaQ8x"
   },
   "outputs": [],
   "source": [
    "df_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72g9zgceaQ8x"
   },
   "outputs": [],
   "source": [
    "corrmat = np.corrcoef(df_train_pca.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6vUthw9aQ8x"
   },
   "outputs": [],
   "source": [
    "corrmat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84y04nFVaQ8x"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15,15])\n",
    "sns.heatmap(corrmat, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6z8uyagaQ8x"
   },
   "outputs": [],
   "source": [
    "df_test_pca = pca_final.transform(X_test)\n",
    "df_test_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2q0KAHdaQ8x"
   },
   "source": [
    "### Applying logistic regression on the data on our Principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hld22Mx_aQ8x"
   },
   "outputs": [],
   "source": [
    "learner_pca = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KenZvF8_aQ8y"
   },
   "outputs": [],
   "source": [
    "model_pca = learner_pca.fit(df_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZDOLn75aQ8y"
   },
   "outputs": [],
   "source": [
    "pred_probs_test = model_pca.predict_proba(df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8Xc40eLaQ8y"
   },
   "outputs": [],
   "source": [
    "y_train_pred = model_pca.predict_proba(df_train_pca)[:,1]\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfNVwq0KaQ8y"
   },
   "outputs": [],
   "source": [
    "y_train_pred_final = pd.DataFrame({'Churn':y_train, 'Churn_Prob':y_train_pred})\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_W8j6t5aQ8y"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let's create columns with different probability cutoffs\n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "\n",
    "for i in numbers:\n",
    "    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kstrUc9vaQ8y"
   },
   "outputs": [],
   "source": [
    "### Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n",
    "\n",
    "# TP = confusion[1,1] # true positive\n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "\n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0n_NuaLFaQ8y"
   },
   "outputs": [],
   "source": [
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yv852XeDaQ8z"
   },
   "outputs": [],
   "source": [
    "### From the curve above, 0.53 is the optimum point to take it as a cutoff probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOlbs7A5aQ8z"
   },
   "outputs": [],
   "source": [
    "y_train_pred_final['final_predicted'] = y_train_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cm_T9hOaQ8z"
   },
   "outputs": [],
   "source": [
    "confusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.final_predicted )\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rd2hwA2iaQ8z"
   },
   "outputs": [],
   "source": [
    "#Making prediction on the test data\n",
    "pred_probs_test = model_pca.predict_proba(df_test_pca)[:,1]\n",
    "y_test_df=pd.DataFrame(y_test)\n",
    "y_pred_df=pd.DataFrame(pred_probs_test)\n",
    "y_test_df.reset_index(drop=True, inplace=True)\n",
    "y_pred_df.reset_index(drop=True, inplace=True)\n",
    "y_test_pred_final=pd.concat([y_test_df, y_pred_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLmCT8Q6aQ8z"
   },
   "outputs": [],
   "source": [
    "\"{:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yT9jbi4haQ8z"
   },
   "outputs": [],
   "source": [
    "# Renaming the column\n",
    "y_test_pred_final= y_test_pred_final.rename(columns={ 0 : 'Churn_prob'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOOVunxlaQ8z"
   },
   "outputs": [],
   "source": [
    "y_test_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DyYJxPORaQ80"
   },
   "outputs": [],
   "source": [
    "y_test_pred_final['final_predicted'] = y_test_pred_final.Churn_prob.map(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFN87QueaQ80"
   },
   "outputs": [],
   "source": [
    "y_test_pred_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ie0IhZ1yaQ80"
   },
   "outputs": [],
   "source": [
    "#y_test_pred_final['churn']=pd.to_numeric(y_test_pred_final['churn'])\n",
    "#y_test_pred_final.info()\n",
    "confusion2 = metrics.confusion_matrix(y_test_pred_final.churn, y_test_pred_final.final_predicted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBXdR8RhaQ80"
   },
   "outputs": [],
   "source": [
    "confusion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZKVgcqjaQ80"
   },
   "outputs": [],
   "source": [
    "TP = confusion2[1,1] # true positive\n",
    "TN = confusion2[0,0] # true negatives\n",
    "FP = confusion2[0,1] # false positives\n",
    "FN = confusion2[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UFGiLddgaQ80"
   },
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "TP / float(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qz57HqS0aQ80"
   },
   "outputs": [],
   "source": [
    "# Positive predictive value\n",
    "print (TP / float(TP+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJPoLHi0aQ81"
   },
   "outputs": [],
   "source": [
    "# Negative predictive value\n",
    "print (TN / float(TN+ FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MpYeuQ5aQ81"
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve( y_test, pred_probs_test, drop_intermediate = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csoe3clhaQ81"
   },
   "outputs": [],
   "source": [
    "#ROC curve code snippet from external source(Module notes)\n",
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0r6zaw8aQ81"
   },
   "outputs": [],
   "source": [
    "draw_roc(y_test, pred_probs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TveBGpiaQ81"
   },
   "source": [
    "## Using Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jfGOHR4oaQ81"
   },
   "outputs": [],
   "source": [
    "# Running the random forest with default parameters.\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(df_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bS6Wfdx1aQ82"
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "predictions = rfc.predict(df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Fjs-MenaQ82"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r273q25SaQ82"
   },
   "source": [
    "####  GridSearchCV to find optimal min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNPGvZJ8aQ82"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create the parameter grid based on the results of random search\n",
    "param = {\n",
    "    'max_depth': [1, 2, 5, 10, 20],\n",
    "    'min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "    'max_features': [2,3,4],\n",
    "    'n_estimators': [10, 30, 50, 100, 200,300]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf,param_grid = param,scoring= 'roc_auc',\n",
    "                          cv = 4, n_jobs = -1,verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(df_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPvAxKf5aQ82"
   },
   "source": [
    "#### printing the optimal accuracy score and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSQrXoHxaQ83"
   },
   "outputs": [],
   "source": [
    "print('We can get best score of',grid_search.best_score_,'using',grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRHY96wjaQ83"
   },
   "outputs": [],
   "source": [
    "# model with the best hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(bootstrap=True,\n",
    "                             max_depth=20,\n",
    "                             min_samples_leaf=5,\n",
    "                             min_samples_split=200,\n",
    "                             max_features=4,\n",
    "                             n_estimators=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0KCp3C_7aQ83"
   },
   "outputs": [],
   "source": [
    "# fit\n",
    "rfc.fit(df_train_pca,y_train)\n",
    "# predict\n",
    "predictions = rfc.predict(df_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "anKoTbfYaQ84"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DP4zwQPKaQ84"
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "print(metrics.confusion_matrix(y_test, predictions), \"\\n\")\n",
    "print(\"accuracy\", metrics.accuracy_score(y_test, predictions))\n",
    "print(\"precision\", metrics.precision_score(y_test, predictions))\n",
    "print(\"sensitivity/recall\", metrics.recall_score(y_test, predictions))\n",
    "print(\"roc_auc_score\", metrics.roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWz-UNwlaQ84"
   },
   "source": [
    "## The classification regression model is the most effective, with a 77% recall and a ROC value of.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2sUdUMnaQ84"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKdJp8K9aQ84"
   },
   "source": [
    "## Model Building for identifying important predictor attributes which help the business understand indicators of churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fPbCntoaQ84"
   },
   "outputs": [],
   "source": [
    "#y = data_hvc.churn\n",
    "#X = data_hvc.drop('churn', axis=1)\n",
    "# scaling the features\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#X_scaled=scaler.fit_transform(X)\n",
    "#X_scaled_df=pd.DataFrame(X_scaled,columns=X.columns)\n",
    "# setting up testing and training sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, train_size=0.7, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gx71ufqnaQ84"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_e0YCM5aQ85"
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0kcNtMXaQ85"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcIhIrQcaQ85"
   },
   "source": [
    "#### will use RFE for feature reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwRxONuBaQ85"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLp2RB2zaQ85"
   },
   "outputs": [],
   "source": [
    "rfe = RFE(logreg,n_features_to_select=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KpX9ifMMaQ85"
   },
   "outputs": [],
   "source": [
    "rfe = rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NC4vZ-YXaQ85"
   },
   "outputs": [],
   "source": [
    "rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYf47CL5aQ86"
   },
   "outputs": [],
   "source": [
    "list(zip(X_train.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5t3QXUsTaQ86"
   },
   "outputs": [],
   "source": [
    "col = X_train.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISFewW-aaQ86"
   },
   "outputs": [],
   "source": [
    "X_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6vIOSEgaQ86"
   },
   "source": [
    "##  Assessment with RandomForest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pKUX9Ip_aQ86"
   },
   "outputs": [],
   "source": [
    "# Running the random forest with default parameters.\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train[col],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SS7XkpCMaQ86"
   },
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "predictions = rfc.predict(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNK82hKPaQ86"
   },
   "outputs": [],
   "source": [
    "# Let's check the report of our default model\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLk5_fBHaQ87"
   },
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6j_52nPaQ87"
   },
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search\n",
    "param_grid = {\n",
    "    'max_depth': [8,12,16],\n",
    "    'min_samples_leaf': range(100, 800, 200),\n",
    "    'min_samples_split': range(200, 1000, 200),\n",
    "    'n_estimators': [100,200, 300],\n",
    "    'max_features': [6,9,12]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid,scoring= 'roc_auc',\n",
    "                          cv = 3, n_jobs = -1,verbose = 1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train[col],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEfUGUJ3aQ87"
   },
   "outputs": [],
   "source": [
    "# printing the optimal accuracy score and hyperparameters\n",
    "print('We can get best score of',grid_search.best_score_,'using',grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baVabqV8aQ87"
   },
   "outputs": [],
   "source": [
    "\n",
    "# model with the best hyperparameters\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(bootstrap=True,\n",
    "                             max_depth=16,\n",
    "                             min_samples_leaf=100,\n",
    "                             min_samples_split=200,\n",
    "                             max_features=9,\n",
    "                             n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbF0jCUcaQ87"
   },
   "outputs": [],
   "source": [
    "# fit\n",
    "rfc.fit(X_train[col],y_train)\n",
    "# predict\n",
    "predictions = rfc.predict(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bMd75EBbaQ87"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xRkm1lr7aQ87"
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "print(metrics.confusion_matrix(y_test, predictions), \"\\n\")\n",
    "print(\"accuracy\", metrics.accuracy_score(y_test, predictions))\n",
    "print(\"precision\", metrics.precision_score(y_test, predictions))\n",
    "print(\"sensitivity/recall\", metrics.recall_score(y_test, predictions))\n",
    "print(\"roc_auc_score\", metrics.roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6wmzFEYaQ88"
   },
   "source": [
    "#### We are getting sensitivity/Recall 61% which is very low. So we will try a different model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcMARh43aQ88"
   },
   "source": [
    "## Logistical Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHfdbS64aQ88"
   },
   "outputs": [],
   "source": [
    "y_train.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04JYyLIHaQ88"
   },
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True,inplace=True)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZ-VPXhEaQ88"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnrJkB_9aQ88"
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-i6gJZsaQ89"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59P7KkNQaQ89"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(logreg,n_features_to_select=20)\n",
    "rfe = rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOx3ij_daQ89"
   },
   "outputs": [],
   "source": [
    "rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfREkIPZaQ89"
   },
   "outputs": [],
   "source": [
    "list(zip(X_train.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAnS9koEaQ89"
   },
   "outputs": [],
   "source": [
    "col = X_train.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1FPC9gNaQ89"
   },
   "outputs": [],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhhigq6oaQ89"
   },
   "outputs": [],
   "source": [
    "X_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "di3kQSo4aQ8-"
   },
   "source": [
    "#### Assessing the model with StatsModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuvC1hnmaQ8-"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmPh8UTmaQ8-"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exjDr21vaQ8_"
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4DanotEaQ8_"
   },
   "outputs": [],
   "source": [
    "y_train.reset_index(drop=True,inplace=True)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ruZekbTwaQ8_"
   },
   "outputs": [],
   "source": [
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQinY6XpaQ8_"
   },
   "outputs": [],
   "source": [
    "# Getting the predicted values on the train set\n",
    "y_train_pred = res.predict(X_train_sm)\n",
    "y_train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14sUNCxIaQ8_"
   },
   "outputs": [],
   "source": [
    "y_train_pred = np.array(y_train_pred).reshape(-1)\n",
    "y_train_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9z_zQJ4jaQ8_"
   },
   "outputs": [],
   "source": [
    "y_train_pred_final = pd.DataFrame({'Churn':y_train, 'Churn_Prob':y_train_pred})\n",
    "y_train_pred_final['CustID'] = y_train.index\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FL1yDxveaQ8_"
   },
   "outputs": [],
   "source": [
    "y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x:1 if x > 0.5 else 0)\n",
    "\n",
    "# Let's see the head\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJaQcHc_aQ9A"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "confusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7bMbltiaQ9A"
   },
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "print(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etffBw30aQ9A"
   },
   "source": [
    "####  Check for the VIF values of the feature variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aR0GwZfaQ9A"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTGyCm8laQ9A"
   },
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNI8KCLFaQ9A"
   },
   "source": [
    "#### drop count_rech_3g_good_phase having High VIF value and recheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPM36XfQaQ9A"
   },
   "outputs": [],
   "source": [
    "col = col.drop('count_rech_3g_good_phase')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_BfZ_-maQ9B"
   },
   "outputs": [],
   "source": [
    "# Let's re-run the model using the selected variables\n",
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm3.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sWsPOl7aQ9B"
   },
   "outputs": [],
   "source": [
    "y_train_pred = res.predict(X_train_sm).values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvb1On5KaQ9B"
   },
   "outputs": [],
   "source": [
    "y_train_pred_final['Churn_Prob'] = y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgoWEkQRaQ9B"
   },
   "outputs": [],
   "source": [
    "# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\n",
    "y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgdYQn4BaQ9B"
   },
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "print(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tL9CQ0K_aQ9B"
   },
   "source": [
    "\n",
    "#### Let's check VIF again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZxfJoBfaQ9B"
   },
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG-IdgekaQ9B"
   },
   "source": [
    "#### P value high for \"monthly_3g_action_phase\"  . Will drop this and remodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNz1xr_RaQ9C"
   },
   "outputs": [],
   "source": [
    "col = col.drop('monthly_3g_action_phase')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqvUcSXOaQ9C"
   },
   "outputs": [],
   "source": [
    "# Let's re-run the model using the selected variables\n",
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm4.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3lEwDPTaQ9C"
   },
   "source": [
    "#### Let's check VIF again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdJX9ZjMaQ9C"
   },
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZqaFs0taQ9C"
   },
   "source": [
    "#### sachet_3g_action_phase is having high P value, we will drop this and remodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yhyr7eXfaQ9C"
   },
   "outputs": [],
   "source": [
    "col = col.drop('sachet_3g_action_phase')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0u1OjSuaQ9C"
   },
   "outputs": [],
   "source": [
    "# Let's re-run the model using the selected variables\n",
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm5.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt-m7MNqaQ9D"
   },
   "source": [
    "#### Now all P values are less than 0.05. Let's check VIF again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GIJbnFjCaQ9D"
   },
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3mNVXJtaQ9D"
   },
   "source": [
    "#### count_rech_2g_good_phase is having highest VIF value. We will drop this and remodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aB1H6-ScaQ9D"
   },
   "outputs": [],
   "source": [
    "col = col.drop('count_rech_2g_good_phase')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7tan-KCaQ9D"
   },
   "outputs": [],
   "source": [
    "# Let's re-run the model using the selected variables\n",
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm6.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a39czZWuaQ9D"
   },
   "source": [
    "#### Checking VIF value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQrk6CH2aQ9E"
   },
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwUXt6xyaQ9E"
   },
   "source": [
    "#### std_og_mou_good_phase is having high VIF, will drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xthwRzBxaQ9E"
   },
   "outputs": [],
   "source": [
    "col = col.drop('std_og_mou_good_phase')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N5XK17hgaQ9E"
   },
   "outputs": [],
   "source": [
    "# Let's re-run the model using the selected variables\n",
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm7 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm7.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4locPqBCaQ9E"
   },
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJf5pfAFaQ9E"
   },
   "source": [
    "#### P value of onnet_mou_good_phase is higher than 0.05, will drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLh-clmgaQ9E"
   },
   "outputs": [],
   "source": [
    "col = col.drop('onnet_mou_good_phase')\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqSghALmaQ9F"
   },
   "outputs": [],
   "source": [
    "# Let's re-run the model using the selected variables\n",
    "X_train_sm = sm.add_constant(X_train[col])\n",
    "logm8 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\n",
    "res = logm8.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y9BnnewOaQ9F"
   },
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train[col].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLkOwW0RaQ9F"
   },
   "outputs": [],
   "source": [
    "coef_dic = {\"fb_user_8\":-1.5011,\"sep_vbc_3g\":-2.0045,\"duration_last_rech_data_8\":0.7331,\"offnet_mou_good_phase\":0.0662,\"loc_og_mou_good_phase\":-1.0688,\n",
    "           \"loc_og_mou_action_phase\":-0.7962,\"spl_ic_mou_good_phase\":-0.7394,\"spl_ic_mou_action_phase\":-0.8440,\"total_ic_mou_good_phase\":-1.3653,\"total_ic_mou_action_phase\":-1.3176,\n",
    "           \"count_rech_3g_action_phase\":-0.1422,\"sachet_2g_good_phase\":0.1007,\"monthly_3g_good_phase\":0.1252,\"sachet_3g_good_phase\":0.0947}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9bAmNOBaQ9F"
   },
   "source": [
    "#### Now P value of all variables are below 0.05 and VIF also less than 5\n",
    "#### We will use this as final model and predict values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzJd8Z0UaQ9F"
   },
   "outputs": [],
   "source": [
    "y_train_pred = res.predict(X_train_sm).values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efJIt4IYaQ9F"
   },
   "outputs": [],
   "source": [
    "y_train_pred_final['Churn_Prob'] = y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xT8JdtoEaQ9F"
   },
   "outputs": [],
   "source": [
    "# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\n",
    "y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59bbTBS-aQ9F"
   },
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "print(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnHU4S8eaQ9G"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbLumIeVaQ9G"
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the confusion matrix again\n",
    "confusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X6sG1yfMaQ9G"
   },
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuA1gIQVaQ9G"
   },
   "outputs": [],
   "source": [
    "TP = confusion[1,1] # true positive\n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpaTzRyzaQ9G"
   },
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "TP / float(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iy8qrrYHaQ9G"
   },
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "TN / float(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSKPZyQKaQ9G"
   },
   "outputs": [],
   "source": [
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print(FP/ float(TN+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxdDAfjDaQ9H"
   },
   "outputs": [],
   "source": [
    "# positive predictive value\n",
    "print (TP / float(TP+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMap3NuuaQ9H"
   },
   "outputs": [],
   "source": [
    "# Negative predictive value\n",
    "print (TN / float(TN+ FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0t8PSQgeaQ9H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZ1UNsPXaQ9O"
   },
   "source": [
    "#### Plot ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFmZm9UNaQ9P"
   },
   "outputs": [],
   "source": [
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cUeZEniaQ9P"
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Churn, y_train_pred_final.Churn_Prob, drop_intermediate = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHh-vJ2raQ9P"
   },
   "outputs": [],
   "source": [
    "draw_roc(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOcm8Wl_aQ9P"
   },
   "source": [
    "####  Finding Optimal Cutoff Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3taQIrFaQ9P"
   },
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs\n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "si3oiGhDaQ9P"
   },
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TP = confusion[1,1] # true positive\n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "\n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_QOQT0yRaQ9Q"
   },
   "outputs": [],
   "source": [
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RDMYT9EaQ9Q"
   },
   "source": [
    "#### select 0.1 as cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUE4ZDbtaQ9Q"
   },
   "outputs": [],
   "source": [
    "y_train_pred_final['final_predicted'] = y_train_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.60 else 0)\n",
    "\n",
    "y_train_pred_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2leaOjZSaQ9Q"
   },
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.final_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJOx6iQmaQ9Q"
   },
   "outputs": [],
   "source": [
    "confusion2 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.final_predicted )\n",
    "confusion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnTB64pRaQ9Q"
   },
   "outputs": [],
   "source": [
    "TP = confusion2[1,1] # true positive\n",
    "TN = confusion2[0,0] # true negatives\n",
    "FP = confusion2[0,1] # false positives\n",
    "FN = confusion2[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "smmEU8zbaQ9Q"
   },
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "TP / float(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NlPB4-uaQ9R"
   },
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "TN / float(TN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pp8K6wGiaQ9R"
   },
   "outputs": [],
   "source": [
    "# Calculate false postive rate - predicting churn when customer does not have churned\n",
    "print(FP/ float(TN+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhpFE3pOaQ9R"
   },
   "outputs": [],
   "source": [
    "# Positive predictive value\n",
    "print (TP / float(TP+FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRGnTE8yaQ9R"
   },
   "outputs": [],
   "source": [
    "# Negative predictive value\n",
    "print (TN / float(TN+ FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZA-HRuRXaQ9R"
   },
   "source": [
    "#### Logical Regression appears to be a superior option due to its high sensitivity (82%), positive predictive value (0.82%), and negative predictive value (0.83%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L55SQqlKaQ9R"
   },
   "source": [
    "##  Now we will use this model to test on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1kiNh611aQ9R"
   },
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "izUrXasLaQ9S"
   },
   "outputs": [],
   "source": [
    "X_test_sm = sm.add_constant(X_test[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ja6p45B6aQ9S"
   },
   "source": [
    "#### Making predictions on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xoIxx4KOaQ9S"
   },
   "outputs": [],
   "source": [
    "y_test_pred = res.predict(X_test_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oUBoX53aQ9S"
   },
   "outputs": [],
   "source": [
    "# Converting y_pred to a dataframe which is an array\n",
    "y_pred_1 = pd.DataFrame(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jg9-JQlqaQ9S"
   },
   "outputs": [],
   "source": [
    "# Let's see the head\n",
    "y_pred_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqIziOh4aQ9S"
   },
   "outputs": [],
   "source": [
    "# Converting y_test to dataframe\n",
    "y_test_df = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrMgMvF3aQ9S"
   },
   "outputs": [],
   "source": [
    "# Putting CustID to index\n",
    "y_test_df['CustID'] = y_test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_86IK2RGaQ9T"
   },
   "outputs": [],
   "source": [
    "# Removing index for both dataframes to append them side by side\n",
    "y_pred_1.reset_index(drop=True, inplace=True)\n",
    "y_test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwfNfyy0aQ9T"
   },
   "outputs": [],
   "source": [
    "# Appending y_test_df and y_pred_1\n",
    "y_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JxPri69aQ9T"
   },
   "outputs": [],
   "source": [
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3Hs1VmeaQ9T"
   },
   "outputs": [],
   "source": [
    "# Renaming the column\n",
    "y_pred_final= y_pred_final.rename(columns={ 0 : 'Churn_Prob'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbPitfMxaQ9T"
   },
   "outputs": [],
   "source": [
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEmNbV_7aQ9T"
   },
   "outputs": [],
   "source": [
    "y_pred_final['final_predicted'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.59 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkBcNZYaaQ9U"
   },
   "outputs": [],
   "source": [
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eflUbVbQaQ9U"
   },
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "metrics.accuracy_score(y_pred_final.churn, y_pred_final.final_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oNa4IBwaQ9U"
   },
   "outputs": [],
   "source": [
    "confusion2 = metrics.confusion_matrix(y_pred_final.churn, y_pred_final.final_predicted )\n",
    "confusion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_8IbwpDaQ9U"
   },
   "outputs": [],
   "source": [
    "TP = confusion2[1,1] # true positive\n",
    "TN = confusion2[0,0] # true negatives\n",
    "FP = confusion2[0,1] # false positives\n",
    "FN = confusion2[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDSTF4FKaQ9V"
   },
   "outputs": [],
   "source": [
    "# Let's see the sensitivity of our logistic regression model\n",
    "TP / float(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYVOPw8kaQ9V"
   },
   "outputs": [],
   "source": [
    "# Let us calculate specificity\n",
    "TN / float(TN+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKCsvzQ7aQ9V"
   },
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SxKTq3oaQ9V"
   },
   "outputs": [],
   "source": [
    "out = dict(sorted(coef_dic.items(), key=lambda t: abs(t[1]),reverse=True))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "amqlPrRjaQ9W"
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(out.items(), columns=['Feature', 'Coef'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypRejaMbaQ9W"
   },
   "outputs": [],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tb9Uu6HyaQ9W"
   },
   "source": [
    "#### We will plot the feature and Coef to understand the influence of each features on churn of the customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrwaAIfcaQ9W"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "sns.barplot(x=out_df.Feature,y=out_df.Coef)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FglcfBkDaQ9W"
   },
   "source": [
    "#### We will plot Coefficiants with its absolute values for finding the top 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpZ01QJ6aQ9W",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "sns.barplot(x=out_df.Feature,y=abs(out_df.Coef))\n",
    "plt.xticks(rotation= 75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "726xadK1aQ9W"
   },
   "source": [
    "### The top5 Features which can influence the customer Churn are :\n",
    "### 1. sep_vbc_3g\n",
    "- Volume based cost - when no specific scheme is not purchased and paid as per usage for 3g in the month of September\n",
    "- This has negative coefficant meaning inversly correlated with churn.\n",
    "- Means the more the value, lesser the chance of churn\n",
    "### 2. fb_user_8\n",
    "- Service scheme to avail services of Facebook and similar social networking sites on 8th month\n",
    "- This has negative coefficant meaning inversly correlated with churn.\n",
    "- Means the more the value, lesser the chance of churn\n",
    "### 3. total_ic_mou_good_phase\n",
    "- Total Incoming Calls Minutes of usage - voice calls during the Good phase (6&7 months)\n",
    "- This has negative coefficant meaning inversly correlated with churn.\n",
    "- Means the more the value, lesser the chance of churn\n",
    "### 4. total_ic_mou_action_phase\n",
    "- Total Incoming Calls Minutes of usage - voice calls during the Action phase (8th month)\n",
    "- This has negative coefficant meaning inversly correlated with churn.\n",
    "- Means the more the value, lesser the chance of churn\n",
    "### 5. loc_og_mou_good_phase\n",
    "- Local calls - within same telecom circle out going calls Minutes of usage - voice calls during the Good phase (6&7) months\n",
    "- This has negative coefficant meaning inversly correlated with churn.\n",
    "- Means the more the value, lesser the chance of churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoU7iIYjaQ9X"
   },
   "source": [
    "# Recomendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQjTF5kAaQ9X"
   },
   "source": [
    "### 1.  Offering specific packages or themes for users to access popular services like Facebook, Instagram, and WhatsApp can help decrease the churn rate.\n",
    "\n",
    "### 2.  The introduction of competitive rates for 3g data can potentially decrease customer churn.\n",
    "\n",
    "### 3. Introducing additional call package categories can increase customer acquisition and decrease churn."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
